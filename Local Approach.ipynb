{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression Sentiment Classifier (Local Approach) for Twitter Political Data   \n",
    "This notebook explains in details various steps implemented for the local approach for a Logistic Regression classifier. The steps we've taken have been broken into stages (represented as `<stage number> - <stage name>`)."
   ],
   "id": "cb858c141a3c97f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 - Getting Started\n",
    "In this stage, we:\n",
    "\n",
    "a. Install and import all dependencies required for the challenge.\n",
    "\n",
    "b. Initialize Spark Session.\n",
    "\n",
    "c. Read and cache dataset."
   ],
   "id": "f8fb603f7ddf8440"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.0/317.0 MB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=7f6004f12f8a2eedcea8c1ed830da067dc022c68a9aa8f4a221920789de1bbcc\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.1\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": "pip install pyspark",
   "id": "VVTRsX44hLyK"
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sbw9V7FHhe7u",
    "outputId": "7fb330b2-37dd-4242-c699-e0ac651f51cb"
   },
   "id": "Sbw9V7FHhe7u",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# %cd '/content/drive/My Drive/notebooks'\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4pcVpFUrOHH",
    "outputId": "3cc1c412-0e1b-453a-acb8-638477de1679"
   },
   "id": "O4pcVpFUrOHH",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/My Drive/notebooks\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:40:59.164390Z",
     "start_time": "2024-05-10T20:40:59.161447Z"
    },
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from src.utils.spark import initSparkSession\n",
    "from src.utils.dataset import getPolarity\n",
    "from src.jobs.spark_etl import extract\n",
    "from src.transforms.Preprocessing import CleanTweet, polarityCalculator\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF, StringIndexer, CountVectorizer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5545a60644fb129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:40:59.176887Z",
     "start_time": "2024-05-10T20:40:59.166512Z"
    },
    "id": "a5545a60644fb129",
    "outputId": "a03c63b2-9611-4d0b-c025-8839baa614a1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdf3f66d570>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7314938dd8da:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Preprocessing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Spark Session for ETL job\n",
    "sparkSession = initSparkSession(appName='Preprocessing')\n",
    "sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0a36bb14dea44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:40:59.260616Z",
     "start_time": "2024-05-10T20:40:59.192270Z"
    },
    "id": "fc0a36bb14dea44",
    "outputId": "42426240-584b-4a9e-872c-3cf8d2af588c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[created_at: timestamp, tweet_id: string, tweet: string, likes: float, retweet_count: float, source: string, user_id: string, user_name: string, user_screen_name: string, user_description: string, user_join_date: timestamp, user_followers_count: float, user_location: string, lat: double, long: double, city: string, country: string, continent: string, state: string, state_code: string, collected_at: timestamp, candidate: string]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df = extract(sparkSession)\n",
    "df.cache()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2 - Spark User Defined Function (UDF)\n",
    "In this stage, we implement an UDF for labelling our dataset using a rule-based algorithm. "
   ],
   "id": "f88bf025dfb68c17"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b87522c6fdbf60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:40:59.273868Z",
     "start_time": "2024-05-10T20:40:59.263994Z"
    },
    "id": "d5b87522c6fdbf60"
   },
   "outputs": [],
   "source": [
    "polarity = sparkSession.udf.register('Polarity', lambda record: getPolarity(record))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3 - Pipelines\n",
    "In this stage, we experiment with different pipeline configurations (stages in the pipeline) to analyze which gives us the best results.    "
   ],
   "id": "ac941636d0293f1e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c4d04133ed3612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:40:59.279303Z",
     "start_time": "2024-05-10T20:40:59.275024Z"
    },
    "id": "a7c4d04133ed3612"
   },
   "outputs": [],
   "source": [
    "def pipeline_clean_data():\n",
    "    cleanTweet = CleanTweet()\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"cleaned_tweet\", outputCol=\"cleaned_tweet_words\", pattern=\"[^a-zA-Z0-9_#]\")\n",
    "    stopWordsRemover = StopWordsRemover(inputCol='cleaned_tweet_words', outputCol='cleaned_tweet_nostop')\n",
    "    return Pipeline(stages=[cleanTweet, regexTokenizer, stopWordsRemover])\n",
    "\n",
    "def pipeline_transform_features():\n",
    "    polarityCalc = polarityCalculator()\n",
    "    countVectorizer = CountVectorizer(inputCol='cleaned_tweet_nostop', outputCol='cv')\n",
    "    idf = IDF(inputCol='cv', outputCol=\"features\")\n",
    "    sentimentStringIdx = StringIndexer(inputCol = \"polarity\", outputCol = \"label\")\n",
    "    return Pipeline(stages=[polarityCalc, countVectorizer, idf, sentimentStringIdx])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 - Data Cleaning",
   "id": "b94658e75e4494f1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "958c76bd284933be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:40:59.414870Z",
     "start_time": "2024-05-10T20:40:59.280017Z"
    },
    "id": "958c76bd284933be"
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline_clean_data().fit(df)\n",
    "df = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "321eb53eafafc6ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:41:29.316494Z",
     "start_time": "2024-05-10T20:41:07.270098Z"
    },
    "id": "321eb53eafafc6ef"
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline_transform_features().fit(df)\n",
    "df = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abeb5ad1b80fbdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:41:33.671226Z",
     "start_time": "2024-05-10T20:41:29.318573Z"
    },
    "id": "abeb5ad1b80fbdf",
    "outputId": "78b2869c-f6dc-4e41-a2da-e2172c382972",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-------------+-------------------+----------------------+-------------------+--------------------+------------------------------+------------------+-------------------+----------+--------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|tweet_id              |created_at         |tweet                                                                                                                                                                                                                                                                                                            |likes |retweet_count|source             |user_id               |user_join_date     |user_followers_count|user_location                 |lat               |long               |city      |state               |state_code|tweet                                                                                                                                                                                                                                                                                                            |candidate|cleaned_tweet_nostop                                                                                                                                                                                                                   |polarity|cv                                                                                                                                                                                                                                        |features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |label|\n",
      "+----------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-------------+-------------------+----------------------+-------------------+--------------------+------------------------------+------------------+-------------------+----------+--------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|1.3165296068962304e+18|2020-10-15 00:01:32|Ice Cube is teaming up to work with President Trump in developing The Platinum Plan. #2020election #AfricanAmericans #BlackAmerican #DiamondandSilk #DonaldTrump #featuredonWJLive #politics #race #TWJReports #USnews https://t.co/XFkzWFMbK0                                                                   |14.0  |6.0          |dlvr.it            |2217181338.0          |2013-11-27 04:58:32|74657.0             |United States                 |39.7837304        |-100.4458825       |NULL      |NULL                |NULL      |Ice Cube is teaming up to work with President Trump in developing The Platinum Plan. #2020election #AfricanAmericans #BlackAmerican #DiamondandSilk #DonaldTrump #featuredonWJLive #politics #race #TWJReports #USnews https://t.co/XFkzWFMbK0                                                                   |TRUMP    |[ice, cube, teaming, work, president, trump, developing, platinum, plan, #2020election, #africanamericans, #blackamerican, #diamondandsilk, #donaldtrump, #featuredonwjlive, #politics, #race, #twjreports, #usnews]                   |Neutral |(30011,[4,8,9,78,134,186,402,600,1613,2489,3558,3966,6393,6438,7589,12546,15684,22638,26560],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                               |(30011,[4,8,9,78,134,186,402,600,1613,2489,3558,3966,6393,6438,7589,12546,15684,22638,26560],[2.094157526312349,2.4989490069110394,2.5050280529874214,3.8436943651180573,4.355381686481529,4.483999064303623,5.0571869297845895,5.432671205680818,6.506282192143511,7.094068857045629,7.317212408359839,7.404223785349469,8.010359588919783,8.010359588919783,8.192681145713738,8.703506769479729,8.703506769479729,9.108971877587894,9.108971877587894])                                                                                                                                                                             |0.0  |\n",
      "|1.3165307481203548e+18|2020-10-15 00:06:04|Twazis says they blocked NY Post's #HunterBiden story cuz they block stories with stolen info.\\n\\nLet's test their theory &amp; #Leftist agenda in protecting #Biden with the story by the #NYPost on STOLEN secret audio tapes of #Trump!\\n\\n#LiberalPrivilege #MAGA\\n https://t.co/bWriPoqI0d                  |19.0  |16.0         |Twitter for Android|1.2584711766717686e+18|2020-05-07 18:58:17|20636.0             |South Carolina, USA           |33.687438799999995|-80.4363743        |NULL      |South Carolina      |SC        |Twazis says they blocked NY Post's #HunterBiden story cuz they block stories with stolen info.\\n\\nLet's test their theory &amp; #Leftist agenda in protecting #Biden with the story by the #NYPost on STOLEN secret audio tapes of #Trump!\\n\\n#LiberalPrivilege #MAGA\\n https://t.co/bWriPoqI0d                  |TRUMP    |[twazis, says, blocked, ny, post, #hunterbiden, story, cuz, block, stories, stolen, info, let, test, theory, amp, #leftist, agenda, protecting, #biden, story, #nypost, stolen, secret, audio, tapes, #trump, #liberalprivilege, #maga]|Negative|(30011,[0,1,3,40,57,74,97,286,430,837,884,1052,1520,1812,1876,2134,2228,2454,2534,2578,2640,2865,3927,4395,7967,8183,12780],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(30011,[0,1,3,40,57,74,97,286,430,837,884,1052,1520,1812,1876,2134,2228,2454,2534,2578,2640,2865,3927,4395,7967,8183,12780],[0.7090993287481991,0.93723112437365,2.31270697463912,3.565749467944135,3.726773027059155,3.8832252038746926,3.9614774007744407,9.72095327107707,5.129290223685933,5.72458161424212,5.7947858729153685,6.017929424229578,6.336383155348113,13.248130455599787,6.583243233279639,6.711076604789524,6.757596620424416,6.8576800789813985,6.911747300251674,6.911747300251674,7.029530335908058,7.029530335908058,7.404223785349469,7.499533965153794,8.192681145713738,8.192681145713738,8.703506769479729])|2.0  |\n",
      "|1.316532492795605e+18 |2020-10-15 00:13:00|@CNN @HarrietD428 This is true!\\n\\nIn a related story, #CNN is facing heat for allowing #WolfBlitzer and #MaggieHaberman to continue working for #Trump and help with #2020Election.                                                                                                                             |11.0  |2.0          |Twitter for iPhone |509543707.0           |2012-03-01 01:25:49|36053.0             | Nashville                    |36.1622296        |-86.7743531        |Nashville |Tennessee           |TN        |@CNN @HarrietD428 This is true!\\n\\nIn a related story, #CNN is facing heat for allowing #WolfBlitzer and #MaggieHaberman to continue working for #Trump and help with #2020Election.                                                                                                                             |TRUMP    |[true, related, story, #cnn, facing, heat, allowing, #wolfblitzer, #maggiehaberman, continue, working, #trump, help, #2020election]                                                                                                    |Positive|(30011,[0,78,194,286,332,482,530,613,1688,1919,2433,3800,18262,25238],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                          |(30011,[0,78,194,286,332,482,530,613,1688,1919,2433,3800,18262,25238],[0.7090993287481991,3.8436943651180573,4.5342608990845115,4.860476635538535,4.934584607692257,5.227408079644456,5.371302259304525,5.458313636294156,6.469914547972635,6.666624842218689,6.8576800789813985,7.404223785349469,9.108971877587894,9.108971877587894])                                                                                                                                                                                                                                                                                              |1.0  |\n",
      "|1.3165343833613066e+18|2020-10-15 00:20:31|Another killer @realDonaldTrump ad from @donwinslow...  #Trump #Election2020 https://t.co/48lkIXmFfb                                                                                                                                                                                                             |537.0 |358.0        |Twitter for iPhone |123281100.0           |2010-03-15 15:46:27|82715.0             |NYC                           |40.7127281        |-74.0060152        |New York  |New York            |NY        |Another killer @realDonaldTrump ad from @donwinslow...  #Trump #Election2020 https://t.co/48lkIXmFfb                                                                                                                                                                                                             |TRUMP    |[another, killer, ad, #trump, #election2020]                                                                                                                                                                                           |Neutral |(30011,[0,5,204,1329,17362],[1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                        |(30011,[0,5,204,1329,17362],[0.7090993287481991,2.0545222194549533,4.544623686120057,6.191201145503615,9.108971877587894])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |0.0  |\n",
      "|1.3165367082179502e+18|2020-10-15 00:29:45|California 23rd...vote...for... @KimMangone! It’s time to dump that shameless #Trump cultist Kevin McCarthy... https://t.co/qn6r4IQ6C9                                                                                                                                                                           |58.0  |26.0         |Twitter for iPhone |123281100.0           |2010-03-15 15:46:27|82715.0             |NYC                           |40.7127281        |-74.0060152        |New York  |New York            |NY        |California 23rd...vote...for... @KimMangone! It’s time to dump that shameless #Trump cultist Kevin McCarthy... https://t.co/qn6r4IQ6C9                                                                                                                                                                           |TRUMP    |[california, 23rd, vote, time, dump, shameless, #trump, cultist, kevin, mccarthy]                                                                                                                                                      |Neutral |(30011,[0,10,35,1010,2356,5973,8341,12274,14504,15972],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                         |(30011,[0,10,35,1010,2356,5973,8341,12274,14504,15972],[0.7090993287481991,2.721251161457057,3.499500082402934,5.95197145643778,6.8576800789813985,7.856208909092526,8.192681145713738,8.703506769479729,8.703506769479729,8.703506769479729])                                                                                                                                                                                                                                                                                                                                                                                        |0.0  |\n",
      "|1.3165396011216937e+18|2020-10-15 00:41:15|@86_45_in2018 @JoeBiden My 18 year old sun just voided your vote.  \\n\\nTexas will reject #socialism \\n\\nStay free #vote for #Trump                                                                                                                                                                               |13.0  |3.0          |Twitter Web App    |7.159702622992548e+17 |2016-04-01 18:33:07|14036.0             |Texas, USA                    |31.8160381        |-99.51209859999999 |NULL      |Texas               |TX        |@86_45_in2018 @JoeBiden My 18 year old sun just voided your vote.  \\n\\nTexas will reject #socialism \\n\\nStay free #vote for #Trump                                                                                                                                                                               |TRUMP    |[18, year, old, sun, voided, vote, texas, reject, #socialism, stay, free, #vote, #trump]                                                                                                                                               |Positive|(30011,[0,10,19,220,336,403,428,429,822,2511,2977,3833,17898],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                      |(30011,[0,10,19,220,336,403,428,429,822,2511,2977,3833,17898],[0.7090993287481991,2.721251161457057,2.9509341306540966,4.609162207257629,4.989934702775422,5.148158707990316,5.167390069918204,5.227408079644456,5.7947858729153685,7.029530335908058,7.094068857045629,7.404223785349469,9.108971877587894])                                                                                                                                                                                                                                                                                                                         |1.0  |\n",
      "|1.3165398596081828e+18|2020-10-15 00:42:17|Trump 2020 highway caravan overflowing this rest stop, one of many planned ahead of the election as America closes in on three weeks to November 3rd #Trump #MAGA #Trump2020 #Michigan https://t.co/k3PKk8Lg4l                                                                                                   |709.0 |255.0        |Twitter for iPhone |23951440.0            |2009-03-12 14:05:32|74290.0             |United States                 |39.7837304        |-100.4458825       |NULL      |NULL                |NULL      |Trump 2020 highway caravan overflowing this rest stop, one of many planned ahead of the election as America closes in on three weeks to November 3rd #Trump #MAGA #Trump2020 #Michigan https://t.co/k3PKk8Lg4l                                                                                                   |TRUMP    |[trump, 2020, highway, caravan, overflowing, rest, stop, one, many, planned, ahead, election, america, closes, three, weeks, november, 3rd, #trump, #maga, #trump2020, #michigan]                                                      |Positive|(30011,[0,4,13,25,30,38,40,86,93,133,180,323,460,549,595,717,925,1913,2460,4993,7133,20547],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                    |(30011,[0,4,13,25,30,38,40,86,93,133,180,323,460,549,595,717,925,1913,2460,4993,7133,20547],[0.7090993287481991,2.094157526312349,2.8686960324171245,3.2036100295333236,3.398544860213024,3.505009738213904,3.565749467944135,3.9820361277954777,4.024466734925182,4.268729569420319,4.413047328331339,4.919317135561468,5.227408079644456,5.336210939493255,5.432671205680818,5.643235974788167,5.850875339566412,6.583243233279639,6.8576800789813985,7.7226775164680035,8.010359588919783,9.108971877587894])                                                                                                                      |1.0  |\n",
      "|1.3165404444032614e+18|2020-10-15 00:44:36|@MollyBeck @patrickdmarley #Trump is president. I have a hard time believing anyone can be that offended over a cuss word at this point.                                                                                                                                                                         |15.0  |1.0          |Twitter for Android|35540152.0            |2009-04-26 19:22:04|3166.0              |Wisconsin, USA                |44.4308975        |-89.6884637        |NULL      |Wisconsin           |WI        |@MollyBeck @patrickdmarley #Trump is president. I have a hard time believing anyone can be that offended over a cuss word at this point.                                                                                                                                                                         |TRUMP    |[#trump, president, hard, time, believing, anyone, offended, cuss, word, point]                                                                                                                                                        |Negative|(30011,[0,8,35,211,300,302,755,4690,6819,24567],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                |(30011,[0,8,35,211,300,302,755,4690,6819,24567],[0.7090993287481991,2.4989490069110394,3.499500082402934,4.587183300538854,4.811686471369103,4.846292000546579,5.643235974788167,7.60489448081162,8.010359588919783,9.108971877587894])                                                                                                                                                                                                                                                                                                                                                                                               |2.0  |\n",
      "|1.316540849036161e+18 |2020-10-15 00:46:13|#Trump: “Los venezolanos verán cosas buenas pasar, siempre estaré con ellos”.  https://t.co/T1FDSD3uRj                                                                                                                                                                                                           |16.0  |6.0          |Twitter Web App    |88537980.0            |2009-11-08 23:26:19|223483.0            |Washington, DC                |38.8949924        |-77.0365581        |Washington|District of Columbia|DC        |#Trump: “Los venezolanos verán cosas buenas pasar, siempre estaré con ellos”.  https://t.co/T1FDSD3uRj                                                                                                                                                                                                           |TRUMP    |[#trump, los, venezolanos, ver, n, cosas, buenas, pasar, siempre, estar, con, ellos]                                                                                                                                                   |Neutral |(30011,[0,29,55,132,2413,2687,3827,4538,4778,5660,7107,9805],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                           |(30011,[0,29,55,132,2413,2687,3827,4538,4778,5660,7107,9805],[0.7090993287481991,3.599583540959917,3.896757210093269,4.3049508328546375,6.911747300251674,6.968905714091623,7.404223785349469,7.60489448081162,7.60489448081162,7.856208909092526,8.010359588919783,8.415824697027949])                                                                                                                                                                                                                                                                                                                                               |0.0  |\n",
      "|1.316542762615935e+18 |2020-10-15 00:53:49|#Trump #Abortion #AP666 https://t.co/5tzLS9GdfA                                                                                                                                                                                                                                                                  |22.0  |3.0          |Twitter for iPhone |3911707993.0          |2015-10-16 07:58:53|24520.0             |Hell is in the mind .New York |40.7127281        |-74.0060152        |New York  |New York            |NY        |#Trump #Abortion #AP666 https://t.co/5tzLS9GdfA                                                                                                                                                                                                                                                                  |TRUMP    |[#trump, #abortion, #ap666]                                                                                                                                                                                                            |Neutral |(30011,[0,7830,22259],[1.0,1.0,1.0])                                                                                                                                                                                                      |(30011,[0,7830,22259],[0.7090993287481991,8.192681145713738,9.108971877587894])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |0.0  |\n",
      "|1.316543798227411e+18 |2020-10-15 00:57:56|@WalshFreedom @GOP Cult of #trump\\n\\n#MontyPython https://t.co/V7ZdRWmGEA                                                                                                                                                                                                                                        |23.0  |7.0          |Twitter Web App    |8.743729821347594e+17 |2017-06-12 21:08:58|84273.0             |New York, USA                 |40.7127281        |-74.0060152        |New York  |New York            |NY        |@WalshFreedom @GOP Cult of #trump\\n\\n#MontyPython https://t.co/V7ZdRWmGEA                                                                                                                                                                                                                                        |TRUMP    |[cult, #trump, #montypython]                                                                                                                                                                                                           |Neutral |(30011,[0,1622,13427],[1.0,1.0,1.0])                                                                                                                                                                                                      |(30011,[0,1622,13427],[0.7090993287481991,6.434823228161365,8.703506769479729])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |0.0  |\n",
      "|1.316544633959338e+18 |2020-10-15 01:01:15|@Waitwha85766617 #Trump already has a library. It's the warehouse where all the copies of Art of the Deal that he purchased are stored. https://t.co/4b9CjoNn3z                                                                                                                                                  |28.0  |1.0          |Twitter Web App    |1.1059268941160572e+18|2019-03-13 20:21:33|14328.0             |Dissident In Florida, USA     |27.7567667        |-81.4639835        |NULL      |Florida             |FL        |@Waitwha85766617 #Trump already has a library. It's the warehouse where all the copies of Art of the Deal that he purchased are stored. https://t.co/4b9CjoNn3z                                                                                                                                                  |TRUMP    |[#trump, already, library, warehouse, copies, art, deal, purchased, stored]                                                                                                                                                            |Neutral |(30011,[0,195,417,1944,4766,4997,6881,13024,14602],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                 |(30011,[0,195,417,1944,4766,4997,6881,13024,14602],[0.7090993287481991,4.51891532940985,5.138679964035772,6.757596620424416,7.60489448081162,7.7226775164680035,8.010359588919783,8.703506769479729,8.703506769479729])                                                                                                                                                                                                                                                                                                                                                                                                               |0.0  |\n",
      "|1.316545406591959e+18 |2020-10-15 01:04:19|@joncoopertweets @NBCNews @ABC Here’s an idea, since @NBCNews has decided to give #DonaldTrump town hall coverage across three channels, @ABC @CBSNews @CNNPolitics @bpolitics @business should carry @ABCPolitics town hall with @JoeBiden. #BidenTownHall #BidenHarrisLandslide2020 #BoycottNBC #2020Elections |21.0  |6.0          |Twitter for iPhone |146572426.0           |2010-05-21 19:49:23|487.0               |Denver, CO                    |39.7392364        |-104.98486229999999|Denver    |Colorado            |CO        |@joncoopertweets @NBCNews @ABC Here’s an idea, since @NBCNews has decided to give #DonaldTrump town hall coverage across three channels, @ABC @CBSNews @CNNPolitics @bpolitics @business should carry @ABCPolitics town hall with @JoeBiden. #BidenTownHall #BidenHarrisLandslide2020 #BoycottNBC #2020Elections |TRUMP    |[idea, since, decided, give, #donaldtrump, town, hall, coverage, across, three, channels, carry, town, hall, #bidentownhall, #bidenharrislandslide2020, #boycottnbc, #2020elections]                                                   |Neutral |(30011,[9,208,231,513,682,695,717,832,889,915,1004,1218,1643,2479,4238,4567],[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                           |(30011,[9,208,231,513,682,695,717,832,889,915,1004,1218,1643,2479,4238,4567],[2.5050280529874214,4.571010441293253,4.620335507855754,10.718935603315046,5.539439181106523,11.255463576504404,5.643235974788167,5.74167604760142,5.77676736741269,5.813135011583565,5.930918047239948,6.088546991443532,6.434823228161365,6.8576800789813985,7.499533965153794,7.60489448081162])                                                                                                                                                                                                                                                      |0.0  |\n",
      "|1.316546127072768e+18 |2020-10-15 01:07:11|I will not like, retweet or reply to any tweets that refer to #Trump's unqualified &amp; absurd #SCOTUS nominee by her initials, no matter how clever they may be.                                                                                                                                               |39.0  |4.0          |Twitter Web App    |1.1059268941160572e+18|2019-03-13 20:21:33|14328.0             |Dissident In Florida, USA     |27.7567667        |-81.4639835        |NULL      |Florida             |FL        |I will not like, retweet or reply to any tweets that refer to #Trump's unqualified &amp; absurd #SCOTUS nominee by her initials, no matter how clever they may be.                                                                                                                                               |TRUMP    |[like, retweet, reply, tweets, refer, #trump, unqualified, amp, absurd, #scotus, nominee, initials, matter, clever, may]                                                                                                               |Negative|(30011,[0,3,16,161,362,432,556,1268,1920,2535,5628,5861,10702,13231,21786],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                 |(30011,[0,3,16,161,362,432,556,1268,1920,2535,5628,5861,10702,13231,21786],[0.7090993287481991,2.31270697463912,2.984288486693689,4.386018655943419,5.006328512551098,5.129290223685933,5.371302259304525,6.164532898421453,6.583243233279639,6.911747300251674,7.856208909092526,7.856208909092526,8.415824697027949,8.703506769479729,9.108971877587894])                                                                                                                                                                                                                                                                           |2.0  |\n",
      "|1.316546586227376e+18 |2020-10-15 01:09:01|\"First the corporate hack-media refused to cover the #Assange story, which would have been their best legitimate shot of hitting Trump effectively for jailing a legit journalist. The same journalist that the legacy media idiocracy blamed for \"\"installing\"\" #Trump to begin with.\"                          |61.0  |27.0         |Twitter Web App    |8.415090420234609e+17 |2017-03-14 04:39:24|23362.0             |United States                 |39.7837304        |-100.4458825       |NULL      |NULL                |NULL      |\"First the corporate hack-media refused to cover the #Assange story, which would have been their best legitimate shot of hitting Trump effectively for jailing a legit journalist. The same journalist that the legacy media idiocracy blamed for \"\"installing\"\" #Trump to begin with.\"                          |TRUMP    |[first, corporate, hack, media, refused, cover, #assange, story, best, legitimate, shot, hitting, trump, effectively, jailing, legit, journalist, journalist, legacy, media, idiocracy, blamed, installing, #trump, begin]             |Positive|(30011,[0,4,85,107,275,286,1013,1108,1441,1852,2293,2549,2950,3390,3661,3721,3771,3951,14056,15131,25150,26382,27131],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |(30011,[0,4,85,107,275,286,1013,1108,1441,1852,2293,2549,2950,3390,3661,3721,3771,3951,14056,15131,25150,26382,27131],[0.7090993287481991,2.094157526312349,3.9585746411164795,8.285273684776437,4.76516645573421,4.860476635538535,5.930918047239948,6.017929424229578,6.305611496681359,6.666624842218689,6.806386784593848,6.911747300251674,14.326123457065162,7.317212408359839,7.317212408359839,7.317212408359839,7.317212408359839,7.404223785349469,8.703506769479729,8.703506769479729,9.108971877587894,9.108971877587894,9.108971877587894])                                                                              |1.0  |\n",
      "|1.3165466968648172e+18|2020-10-15 01:09:27|#Iowa experienced severe devastation from the recent #Derecho. And #Florida #Panhandle suffered significant #hurricane damage. But, #POTUS #Trump tells #Iowans he’s upset that TV news talked about those catastrophes instead of his “Nobel Prize nomination.” Is it all about him? https://t.co/1TV0ecQ88Z    |26.0  |10.0         |Twitter for iPhone |47253430.0            |2009-06-15 03:54:18|5468.0              |Mobile, AL                    |30.6943566        |-88.04305409999999 |Mobile    |Alabama             |AL        |#Iowa experienced severe devastation from the recent #Derecho. And #Florida #Panhandle suffered significant #hurricane damage. But, #POTUS #Trump tells #Iowans he’s upset that TV news talked about those catastrophes instead of his “Nobel Prize nomination.” Is it all about him? https://t.co/1TV0ecQ88Z    |TRUMP    |[#iowa, experienced, severe, devastation, recent, #derecho, #florida, #panhandle, suffered, significant, #hurricane, damage, #potus, #trump, tells, #iowans, upset, tv, news, talked, catastrophes, instead, nobel, prize, nomination] |Positive|(30011,[0,91,200,411,550,587,800,1339,1428,2186,2192,2236,2336,2997,3672,3674,4956,5216,15811,17721,22228,24109,26847,27307,28944],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |(30011,[0,91,200,411,550,587,800,1339,1428,2186,2192,2236,2336,2997,3672,3674,4956,5216,15811,17721,22228,24109,26847,27307,28944],[0.7090993287481991,3.9614774007744407,4.5088142334233465,5.083620186852745,5.359467801657523,5.420092423473958,5.674984673102748,6.218600119691729,6.246770996658426,6.757596620424416,6.911747300251674,6.757596620424416,6.806386784593848,7.094068857045629,7.317212408359839,7.317212408359839,7.7226775164680035,7.7226775164680035,8.703506769479729,9.108971877587894,9.108971877587894,9.108971877587894,9.108971877587894,9.108971877587894,9.108971877587894])                          |1.0  |\n",
      "|1.3165491477433915e+18|2020-10-15 01:19:11|Now #Twitter and #Facebook are censoring the NYPost's story to a never-before-seen-degree. Why? I imagine, to protect themselves from being hauled in front of Congress for InstallingTrumpforPutin2.0™ - should #Trump win.                                                                                     |54.0  |27.0         |Twitter Web App    |8.415090420234609e+17 |2017-03-14 04:39:24|23362.0             |United States                 |39.7837304        |-100.4458825       |NULL      |NULL                |NULL      |Now #Twitter and #Facebook are censoring the NYPost's story to a never-before-seen-degree. Why? I imagine, to protect themselves from being hauled in front of Congress for InstallingTrumpforPutin2.0™ - should #Trump win.                                                                                     |TRUMP    |[#twitter, #facebook, censoring, nypost, story, never, seen, degree, imagine, protect, hauled, front, congress, installingtrumpforputin2, 0, #trump, win]                                                                              |Positive|(30011,[0,23,82,286,374,546,567,796,841,1246,1677,2069,3187,7877,9414,20265,25976],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                 |(30011,[0,23,82,286,374,546,567,796,841,1246,1677,2069,3187,7877,9414,20265,25976],[0.7090993287481991,3.192769814980459,3.91879666965956,4.860476635538535,4.989934702775422,5.359467801657523,5.445410231458247,5.7947858729153685,5.72458161424212,6.1385574120181925,6.506282192143511,6.711076604789524,7.163061728532581,8.192681145713738,8.415824697027949,9.108971877587894,9.108971877587894])                                                                                                                                                                                                                              |1.0  |\n",
      "|1.3165493286773432e+18|2020-10-15 01:19:54|If @CBS is willing, I'd be happy to do a #TownHall on their network beginning at 8 pm Thursday night. The country has already heard what #Trump &amp; #Biden have to offer &amp; would probably love to learn about my vision to build a nation where We the People truly means #AllThePeople.                   |180.0 |53.0         |Twitter for Android|28926581.0            |2009-04-05 02:53:09|16092.0             |Washington, DC                |38.8949924        |-77.0365581        |Washington|District of Columbia|DC        |If @CBS is willing, I'd be happy to do a #TownHall on their network beginning at 8 pm Thursday night. The country has already heard what #Trump &amp; #Biden have to offer &amp; would probably love to learn about my vision to build a nation where We the People truly means #AllThePeople.                   |TRUMP    |[willing, d, happy, #townhall, network, beginning, 8, pm, thursday, night, country, already, heard, #trump, amp, #biden, offer, amp, probably, love, learn, vision, build, nation, people, truly, means, #allthepeople]                |Positive|(30011,[0,1,3,15,36,46,140,144,195,243,262,440,538,548,586,612,1016,1173,1236,1308,1528,1544,1573,1708,2733,3201,3524],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])     |(30011,[0,1,3,15,36,46,140,144,195,243,262,440,538,548,586,612,1016,1173,1236,1308,1528,1544,1573,1708,2733,3201,3524],[0.7090993287481991,0.93723112437365,4.62541394927824,2.9626426199189964,3.638804254113198,3.6557898004789418,4.288690311982857,4.313181331991153,4.51891532940985,4.7207146931633766,4.7207146931633766,5.206999208013249,5.336210939493255,5.359467801657523,5.4076699034754006,5.471385717861508,5.95197145643778,6.113239604033903,6.1385574120181925,6.246770996658426,6.368131853662693,6.400921676485684,6.368131853662693,6.469914547972635,6.968905714091623,7.163061728532581,7.404223785349469])    |1.0  |\n",
      "|1.3165498195996426e+18|2020-10-15 01:21:51|THIS is UNCONSCIONABLE. #Trump pulled out of the scheduled townhall. #Biden KEPT HIS COMMITMENT &amp; HIS WORD and moved forward. Now @nbc @nbcnews is going to REWARD 45 for breaking his commitment and be an accomplice to his attempt to screw over Biden?! #BoycottNBC #BoycottMSNBC https://t.co/8v4AGGUZ4G|6431.0|2384.0       |Twitter for iPhone |499073990.0           |2012-02-21 19:01:16|664069.0            |New York City                 |40.7127281        |-74.0060152        |New York  |New York            |NY        |THIS is UNCONSCIONABLE. #Trump pulled out of the scheduled townhall. #Biden KEPT HIS COMMITMENT &amp; HIS WORD and moved forward. Now @nbc @nbcnews is going to REWARD 45 for breaking his commitment and be an accomplice to his attempt to screw over Biden?! #BoycottNBC #BoycottMSNBC https://t.co/8v4AGGUZ4G|TRUMP    |[unconscionable, #trump, pulled, scheduled, townhall, #biden, kept, commitment, amp, word, moved, forward, going, reward, 45, breaking, commitment, accomplice, attempt, screw, biden, #boycottnbc, #boycottmsnbc]                     |Neutral |(30011,[0,1,3,7,31,170,755,770,805,1499,1551,1745,2085,2902,4567,4892,5023,5654,5701,5822,7113,10961],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])                                          |(30011,[0,1,3,7,31,170,755,770,805,1499,1551,1745,2085,2902,4567,4892,5023,5654,5701,5822,7113,10961],[0.7090993287481991,0.93723112437365,2.31270697463912,2.3200001345957237,3.4287992685708266,4.377169040666437,5.643235974788167,5.643235974788167,5.707774495925738,6.336383155348113,6.368131853662693,6.469914547972635,6.711076604789524,7.094068857045629,7.60489448081162,7.60489448081162,16.020719177839567,7.856208909092526,7.856208909092526,7.856208909092526,8.010359588919783,8.415824697027949])                                                                                                                  |0.0  |\n",
      "|1.316550112005546e+18 |2020-10-15 01:23:01|Fascinating....all the sudden #CNN cares about the wellbeing of Barron #Trump 😒                                                                                                                                                                                                                                 |33.0  |9.0          |Twitter for iPhone |1.02191760904917e+18  |2018-07-25 00:38:39|3733.0              |United States                 |39.7837304        |-100.4458825       |NULL      |NULL                |NULL      |Fascinating....all the sudden #CNN cares about the wellbeing of Barron #Trump 😒                                                                                                                                                                                                                                 |TRUMP    |[fascinating, sudden, #cnn, cares, wellbeing, barron, #trump]                                                                                                                                                                          |Positive|(30011,[0,482,1034,4141,4660,5572,10308],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                   |(30011,[0,482,1034,4141,4660,5572,10308],[0.7090993287481991,5.227408079644456,5.973477661658745,7.499533965153794,7.60489448081162,8.010359588919783,8.415824697027949])                                                                                                                                                                                                                                                                                                                                                                                                                                                             |1.0  |\n",
      "+----------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-------------+-------------------+----------------------+-------------------+--------------------+------------------------------+------------------+-------------------+----------+--------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5 - Train-Test Split",
   "id": "de4110cf7c2a75f9"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59134e683e1282d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:41:33.752850Z",
     "start_time": "2024-05-10T20:41:33.675241Z"
    },
    "id": "59134e683e1282d9"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit([0.70, 0.30], seed=123456)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6 - Access underlying RDD\n",
    "The DataFrame API is not flexible enough for local (divide-and-conquer) approach. We don't do repartitioning as our dataset is large enough. "
   ],
   "id": "584c000c75991b12"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df42ea567cc08c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:41:48.337827Z",
     "start_time": "2024-05-10T20:41:33.754956Z"
    },
    "id": "df42ea567cc08c83"
   },
   "outputs": [],
   "source": [
    "rdd_train = df_train.rdd.cache()\n",
    "rdd_test = df_test.rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rdd_train.getNumPartitions()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsNk3ibsjGKb",
    "outputId": "ca09b8c8-d63d-4eb3-e8df-08ab1cc6693a"
   },
   "id": "JsNk3ibsjGKb",
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rdd_test.getNumPartitions()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1Af2RTOjGC2",
    "outputId": "61638d16-419c-4450-e9e2-fe92ded542f1"
   },
   "id": "j1Af2RTOjGC2",
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "all_columns = df_train.columns\n",
    "all_columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzGb1pGJnpOu",
    "outputId": "ecca3206-8823-4b38-8949-8d7739ffc036"
   },
   "id": "BzGb1pGJnpOu",
   "execution_count": 82,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tweet_id',\n",
       " 'created_at',\n",
       " 'tweet',\n",
       " 'likes',\n",
       " 'retweet_count',\n",
       " 'source',\n",
       " 'user_id',\n",
       " 'user_join_date',\n",
       " 'user_followers_count',\n",
       " 'user_location',\n",
       " 'lat',\n",
       " 'long',\n",
       " 'city',\n",
       " 'state',\n",
       " 'state_code',\n",
       " 'tweet',\n",
       " 'candidate',\n",
       " 'cleaned_tweet_nostop',\n",
       " 'polarity',\n",
       " 'cv',\n",
       " 'features',\n",
       " 'label']"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7 - Building Machine Learning Models in Parallel",
   "id": "cb160e2add91b6e3"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "id": "4W50zQdKve8p"
   },
   "id": "4W50zQdKve8p",
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7cb90bd965f22b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:41:48.340532Z",
     "start_time": "2024-05-10T20:41:48.338416Z"
    },
    "id": "7cb90bd965f22b04"
   },
   "outputs": [],
   "source": [
    "def build_model(it_partition_data):\n",
    "    df_partition_data = pd.DataFrame(it_partition_data, columns=all_columns)\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    X_train = df_partition_data[['features']]\n",
    "    y_train = df_partition_data[['label']]\n",
    "    model = clf.fit(X_train.values, y_train.values)\n",
    "    return [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f0bdc0c5c43d5248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T20:58:28.851191Z",
     "start_time": "2024-05-10T20:58:28.776790Z"
    },
    "id": "f0bdc0c5c43d5248",
    "outputId": "ddbd596e-b736-42b8-c8f8-c6ad61a07559",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 46) (7314938dd8da executor driver): org.apache.spark.api.python.PythonException: TypeError: float() argument must be a string or a real number, not 'SparseVector'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"<ipython-input-89-b8e56ec4728d>\", line 6, in build_model\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1196, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: setting an array element with a sequence.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: TypeError: float() argument must be a string or a real number, not 'SparseVector'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"<ipython-input-89-b8e56ec4728d>\", line 6, in build_model\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1196, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: setting an array element with a sequence.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-90-2bd855a23b52>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuild_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1831\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1832\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1833\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1834\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1835\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1321\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1322\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1323\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1324\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    178\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 179\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    180\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    181\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 46) (7314938dd8da executor driver): org.apache.spark.api.python.PythonException: TypeError: float() argument must be a string or a real number, not 'SparseVector'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"<ipython-input-89-b8e56ec4728d>\", line 6, in build_model\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1196, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: setting an array element with a sequence.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: TypeError: float() argument must be a string or a real number, not 'SparseVector'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"<ipython-input-89-b8e56ec4728d>\", line 6, in build_model\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1196, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: setting an array element with a sequence.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "models = rdd_train.mapPartitions(build_model).collect()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15be9ac22c08079",
   "metadata": {
    "id": "b15be9ac22c08079",
    "outputId": "3f88dba9-23f7-49b8-c72d-8f5eea672fd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(models)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 8 - Results\n",
    "As the error above explains, the scikit-learn library does not handle complex types required for our dataset i.e. a sparse vector representing tweet data, and only accepts Strings or Numbers. This is the main limitation in exploring this approach further."
   ],
   "id": "94fd7ae94788cd4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e7e784961fbaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T18:32:09.940693Z",
     "start_time": "2024-05-10T18:32:08.954332Z"
    },
    "id": "f78e7e784961fbaa"
   },
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617abc21456295b",
   "metadata": {
    "id": "617abc21456295b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
